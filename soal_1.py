# -*- coding: utf-8 -*-
"""soal 1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RLZUmiTIOr5l9UBmlBlfZ31vLjmupwkP
"""

# Install feature_engine library
!pip install feature_engine

"""# **First Step**"""

# Import Libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Load Dataset
df = pd. read_csv("/content/drive/MyDrive/Kampus/Semester 7/Bio Infor/Project/Soal1/classification16.csv")
df.head(5)

df.info()

# rename columns
df.rename({
    'SEQN': 'id',
    'DEMO.RIDAGEYR': 'age',
    'DEMO.RIAGENDR': 'gender',
    'DEMO.RIDRETH1': 'race',
    'DEMO.DMDEDUC2': 'education',
    'DEMO.INDHHINC': 'household_income',
    'DIQ.DIQ010': 'diabetes',
    'LAB10AM.LBXGLU': 'blood_glucose',
    'MCQ.MCQ250A': 'family_history',
    'WHQ.WHD010': 'height',
    'WHQ.WHD020': 'weight',
    'BPQ.BPQ020': 'hypertension',
    'SMQ.SMQ020': 'smoker',
    'ALQ.ALQ100': 'alcohol',
    'BMX.BMXWAIST': 'waist_circumference',
    'BMX.BMXBMI': 'bmi',
    'labels': 'labels'}, axis=1, inplace=True)

"""# **Preprocessing**

## **Drop Outlier**

### **age**
"""

# scatter age
print(df.plot.scatter(x = 'id', y = 'age'))

# boxplot age
sns.boxplot(df.age)

"""### **gender**"""

# scatter gender
df.plot.scatter(x = 'id', y = 'gender');

# boxplot gender
sns.boxplot(df.gender)

"""### **race**"""

# scatter race
df.plot.scatter(x = 'id', y = 'race');

# boxplot race
sns.boxplot(df.race)

"""### **education**"""

# scatter education
df.plot.scatter(x = 'id', y = 'education');

# boxplot education
sns.boxplot(df.education)

a = df[df.education == 9.0].index

for index in range(len(a)):
  df.drop([a[index]], axis=0, inplace=True )

sns.boxplot(df.education)

"""### **household_income**"""

# scatter household_income
df.plot.scatter(x = 'id', y = 'household_income');

# boxplot household_income
sns.boxplot(df.household_income)

b = df[df.household_income > 20].index

for index in range(len(b)):
  df.drop([b[index]], axis=0, inplace=True )

# boxplot household_income
sns.boxplot(df.household_income)

"""### **diabetes**"""

# scatter diabetes
df.plot.scatter(x = 'id', y = 'diabetes');

# boxplot diabetes
sns.boxplot(df.diabetes)

"""### **blood_glucose**"""

# scatter blood_glucose
df.plot.scatter(x = 'id', y = 'blood_glucose')

# boxplot blood_glucose
sns.boxplot(df['blood_glucose'])

c = df[df['blood_glucose'] > 500].index

for index in range(len(c)):
  df.drop([c[index]], axis=0, inplace=True )

# scatter blood glucose
df.plot.scatter(x = 'id', y = 'blood_glucose')

"""### **family_history**"""

# scatter family_history
df.plot.scatter(x = 'id', y = 'family_history')

# boxplot family_history
sns.boxplot(df['family_history'])

"""### **height**"""

# scatter height
df.plot.scatter(x = 'id', y = 'height')

# boxplot height
sns.boxplot(df['height'])

d = df[df['height'] > 200].index

for index in range(len(d)):
  df.drop(d[index], axis=0, inplace=True )

# boxplot height
sns.boxplot(df['height'])

"""### **weight**"""

# scatter weight
df.plot.scatter(x = 'id', y = 'weight')

# boxplot weight
sns.boxplot(df['weight'])

e = df[df['weight'] > 600].index

for index in range(len(e)):
  df.drop(e[index], axis=0, inplace=True )

# boxplot weight
sns.boxplot(df['weight'])

"""### **hypertension**"""

# scatter hypertension
df.plot.scatter(x = 'id', y = 'hypertension')

# boxplot hypertension
sns.boxplot(df['hypertension'])

f = df[df['hypertension'] > 2].index

for index in range(len(f)):
  df.drop(f[index], axis=0, inplace=True )

# boxplot hypertension
sns.boxplot(df['hypertension'])

"""### **smoker**"""

# scatter smoker
df.plot.scatter(x = 'id', y = 'smoker')

# boxplot smoker
sns.boxplot(df['smoker'])

g = df[df['smoker'] > 2].index

for index in range(len(g)):
  df.drop(g[index], axis=0, inplace=True )

# boxplot smoker
sns.boxplot(df['smoker'])

"""### **alcohol**"""

# scatter alcohol
df.plot.scatter(x = 'id', y = 'alcohol')

# boxplot alcohol
sns.boxplot(df['alcohol'])

h = df[df['alcohol'] > 2].index

for index in range(len(h)):
  df.drop(h[index], axis=0, inplace=True )

# boxplot alcohol
sns.boxplot(df['alcohol'])

"""### **waist_circumference**"""

# scatter waist_circumference
df.plot.scatter(x = 'id', y = 'waist_circumference')

# boxplot waist_circumference
sns.boxplot(df['waist_circumference'])

i = df[df['waist_circumference'] > 160].index

for index in range(len(i)):
  df.drop(i[index], axis=0, inplace=True )

# boxplot waist_circumference
sns.boxplot(df['waist_circumference'])

"""### **bmi**"""

# scatter bmi
df.plot.scatter(x = 'id', y = 'bmi')

# boxplot bmi
sns.boxplot(df['bmi'])

i = df[df['bmi'] > 80].index

for index in range(len(i)):
  df.drop(i[index], axis=0, inplace=True )

# boxplot bmi
sns.boxplot(df['bmi'])

"""## **Data Cleaning**"""

df.isnull().sum()

sns.distplot(df['education'])

from sklearn.impute import SimpleImputer

train_most_frequent = df.copy()
#setting strategy to 'most_frequent' to impute by the modus
mean_imputer = SimpleImputer(strategy='most_frequent')# strategy can also be mean or median 
train_most_frequent.iloc[:,:] = mean_imputer.fit_transform(train_most_frequent)
train_most_frequent.isnull().sum()

from sklearn import preprocessing
le = preprocessing.LabelEncoder()
train_most_frequent.labels = le.fit_transform(train_most_frequent.labels)

"""# **Machine Learning**"""

# Preparing Data For Training
X = train_most_frequent.iloc[:, 1:16].values
y = train_most_frequent.iloc[:, 16].values

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

# Feature Scaling
from sklearn.preprocessing import StandardScaler

sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

"""## **Random Forest**"""

# Training the Algorithm
from sklearn.ensemble import RandomForestRegressor

regressor = RandomForestRegressor(n_estimators=30, random_state=0)
regressor.fit(X_train, y_train)
y_pred = regressor.predict(X_test)

# Evaluating the Algorithm
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))
print(accuracy_score(y_test, y_pred))

"""## **SVM**"""

from sklearn import svm

clf = svm.SVC(kernel='linear')
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)

#Import scikit-learn metrics module for accuracy calculation
from sklearn import metrics

# Model Accuracy: how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))

# Model Precision: what percentage of positive tuples are labeled as such?
print("Precision:",metrics.precision_score(y_test, y_pred))

# Model Recall: what percentage of positive tuples are labelled as such?
print("Recall:",metrics.recall_score(y_test, y_pred))

# Evaluating the Algorithm
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))
print(accuracy_score(y_test, y_pred))

"""## **Naive Bayes**"""

#Import Gaussian Naive Bayes model
from sklearn.naive_bayes import GaussianNB

#Create a Gaussian Classifier
gnb = GaussianNB()

#Train the model using the training sets
gnb.fit(X_train, y_train)

#Predict the response for test dataset
y_pred = gnb.predict(X_test)

#Import scikit-learn metrics module for accuracy calculation
from sklearn import metrics

# Model Accuracy, how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))

# Evaluating the Algorithm
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))
print(accuracy_score(y_test, y_pred))

"""## **KNN**"""

from sklearn.neighbors import KNeighborsClassifier
error = []

# Calculating error for K values between 1 and 40
for i in range(1, 40):
    knn = KNeighborsClassifier(n_neighbors=i)
    knn.fit(X_train, y_train)
    pred_i = knn.predict(X_test)
    error.append(np.mean(pred_i != y_test))

plt.figure(figsize=(12, 6))
plt.plot(range(1, 40), error, color='red', linestyle='dashed', marker='o',
         markerfacecolor='blue', markersize=10)
plt.title('Error Rate K Value')
plt.xlabel('K Value')
plt.ylabel('Mean Error')

classifier = KNeighborsClassifier(n_neighbors=6)
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)

# Evaluating the Algorithm
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))
print(accuracy_score(y_test, y_pred))

"""## **ADABOOST**"""

from sklearn.ensemble import AdaBoostClassifier
from sklearn.svm import SVC
svc=SVC(probability=True, kernel='linear')

# Create adaboost classifer object
abc =AdaBoostClassifier(n_estimators=50, base_estimator=svc,learning_rate=1)

# Train Adaboost Classifer
model = abc.fit(X_train, y_train)

#Predict the response for test dataset
y_pred = model.predict(X_test)

# Evaluating the Algorithm
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))
print(accuracy_score(y_test, y_pred))